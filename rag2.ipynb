{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c5476cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\kanth\\anaconda3\\lib\\site-packages (0.3.25)\n",
      "Requirement already satisfied: langchain-ollama in c:\\users\\kanth\\anaconda3\\lib\\site-packages (0.3.8)\n",
      "Collecting langchain-huggingface\n",
      "  Using cached langchain_huggingface-1.0.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\kanth\\anaconda3\\lib\\site-packages (0.3.24)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\kanth\\anaconda3\\lib\\site-packages (1.11.0)\n",
      "Requirement already satisfied: rank-bm25 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (0.2.2)\n",
      "Requirement already satisfied: datasets in c:\\users\\kanth\\anaconda3\\lib\\site-packages (4.4.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kanth\\anaconda3\\lib\\site-packages (4.66.5)\n",
      "Requirement already satisfied: gradio in c:\\users\\kanth\\anaconda3\\lib\\site-packages (5.29.0)\n",
      "Requirement already satisfied: torch in c:\\users\\kanth\\anaconda3\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\kanth\\anaconda3\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from langchain) (0.3.76)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from langchain) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from langchain) (0.3.45)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from langchain) (2.11.5)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from langchain) (2.0.34)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: ollama<1.0.0,>=0.5.3 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from langchain-ollama) (0.5.3)\n",
      "Collecting huggingface-hub<1.0.0,>=0.33.4 (from langchain-huggingface)\n",
      "  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "INFO: pip is looking at multiple versions of langchain-huggingface to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain-huggingface\n",
      "  Downloading langchain_huggingface-1.0.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "  Downloading langchain_huggingface-0.3.1-py3-none-any.whl.metadata (996 bytes)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from langchain-huggingface) (0.21.1)\n",
      "Collecting huggingface-hub>=0.33.4 (from langchain-huggingface)\n",
      "  Using cached huggingface_hub-1.1.4-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from langchain-community) (3.10.5)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from langchain-community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from langchain-community) (2.9.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from faiss-cpu) (25.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from datasets) (0.27.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from gradio) (4.9.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from gradio) (0.115.12)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from gradio) (0.5.0)\n",
      "Requirement already satisfied: gradio-client==1.10.0 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from gradio) (1.10.0)\n",
      "Requirement already satisfied: groovy~=0.1 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from gradio) (2.1.3)\n",
      "Requirement already satisfied: orjson~=3.0 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from gradio) (3.10.18)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from gradio) (10.4.0)\n",
      "Requirement already satisfied: pydub in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: ruff>=0.9.3 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from gradio) (0.11.9)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from gradio) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from gradio) (0.46.2)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from gradio) (0.13.2)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from gradio) (0.15.3)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from gradio) (4.13.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from gradio) (0.34.2)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from gradio-client==1.10.0->gradio) (15.0.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.50.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from httpx<1.0.0->datasets) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (1.2.0)\n",
      "Requirement already satisfied: shellingham in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (1.5.0)\n",
      "Requirement already satisfied: typer-slim in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (0.20.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.4.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (2.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.15.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\kanth\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.0)\n",
      "Downloading langchain_huggingface-0.3.1-py3-none-any.whl (27 kB)\n",
      "Using cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "Installing collected packages: huggingface-hub, langchain-huggingface\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.30.1\n",
      "    Uninstalling huggingface-hub-0.30.1:\n",
      "      Successfully uninstalled huggingface-hub-0.30.1\n",
      "Successfully installed huggingface-hub-0.36.0 langchain-huggingface-0.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain-ollama langchain-huggingface langchain-community faiss-cpu rank-bm25 datasets tqdm gradio torch sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1db5b4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Ollama + Models...\n",
      "WARNING:tensorflow:From c:\\Users\\kanth\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Imports & Setup\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from rank_bm25 import BM25Okapi\n",
    "import gradio as gr\n",
    "\n",
    "print(\"Setting up Ollama + Models...\")\n",
    "llm = ChatOllama(model=\"llama3:latest\", temperature=0.0)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "886f8ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HotpotQA dataset...\n",
      "Processing documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7405/7405 [00:18<00:00, 402.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique documents: 273710 | Questions: 7405\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load HotpotQA dataset\n",
    "print(\"Loading HotpotQA dataset...\")\n",
    "dataset = load_dataset(\"hotpot_qa\", \"fullwiki\", split=\"validation\")\n",
    "\n",
    "documents = []\n",
    "questions = []\n",
    "\n",
    "print(\"Processing documents...\")\n",
    "for item in tqdm(dataset):\n",
    "    questions.append({\n",
    "        \"question\": item[\"question\"],\n",
    "        \"answer\": item[\"answer\"],\n",
    "        \"supporting_titles\": set(item[\"supporting_facts\"][\"title\"])\n",
    "    })\n",
    "    \n",
    "    for title, sentences in zip(item[\"context\"][\"title\"], item[\"context\"][\"sentences\"]):\n",
    "        for sent in sentences:\n",
    "            documents.append(Document(\n",
    "                page_content=sent.strip(),\n",
    "                metadata={\"title\": title}\n",
    "            ))\n",
    "\n",
    "# Remove duplicate sentences\n",
    "unique_docs = {doc.page_content: doc for doc in documents}\n",
    "documents = list(unique_docs.values())\n",
    "print(f\"Total unique documents: {len(documents)} | Questions: {len(questions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b917739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building FAISS + BM25 indexes...\n",
      "Indexes ready!\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Build FAISS + BM25 indexes\n",
    "print(\"Building FAISS + BM25 indexes...\")\n",
    "vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "tokenized_corpus = [doc.page_content.lower().split() for doc in documents]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "print(\"Indexes ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98fee797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Prompts & Chains\n",
    "decompose_template = \"Break this question into 2-3 simple sub-questions:\\nQuestion: {question}\\nSub-questions:\\n1.\"\n",
    "generate_template = \"\"\"Using only the context below, answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "decompose_prompt = PromptTemplate.from_template(decompose_template)\n",
    "generate_prompt = PromptTemplate.from_template(generate_template)\n",
    "\n",
    "decompose_chain = decompose_prompt | llm\n",
    "generate_chain = generate_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a79edd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Hybrid Retrieval Function\n",
    "def retrieve(query, k=8):\n",
    "    # Dense (FAISS)\n",
    "    dense_docs = vectorstore.similarity_search(query, k=k)\n",
    "    # Sparse (BM25)\n",
    "    bm25_scores = bm25.get_scores(query.lower().split())\n",
    "    bm25_docs = [documents[i] for i in bm25_scores.argsort()[::-1][:k]]\n",
    "    \n",
    "    # Merge & deduplicate\n",
    "    seen = set()\n",
    "    merged = []\n",
    "    for doc in dense_docs + bm25_docs:\n",
    "        if doc.page_content not in seen:\n",
    "            seen.add(doc.page_content)\n",
    "            merged.append(doc)\n",
    "    return merged[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e880311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 ko replace kar de iss se (sirf ye part change kar):\n",
    "\n",
    "def multi_hop_rag(question):\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    \n",
    "    # Step 1: Decompose\n",
    "    sub_text = decompose_chain.invoke({\"question\": question}).content\n",
    "    subs = []\n",
    "    for line in sub_text.split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        if line and any(c.isdigit() for c in line[:3]):\n",
    "            sub_q = line.split(\".\", 1)[-1].strip(\" :-\")\n",
    "            if sub_q:\n",
    "                subs.append(sub_q)\n",
    "    \n",
    "    if not subs:\n",
    "        subs = [question]\n",
    "    subs = subs[:3]\n",
    "    print(\"Sub-questions:\", subs)\n",
    "    \n",
    "    # Step 2: Retrieve + Strict context limiting\n",
    "    context_parts = []\n",
    "    for sq in subs:\n",
    "        docs = retrieve(sq, k=5)  # k=6 → k=5 kar diya\n",
    "        context_parts.extend([d.page_content for d in docs])\n",
    "    \n",
    "    # YE LINE SABSE ZAROORI HAI → 8k-10k tokens max\n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    context = context[:24000]  # 32k → 24k (safe zone for 8B on CPU/GPU)\n",
    "    \n",
    "    # Optional: Agar phir bhi crash ho to 16000 kar dena\n",
    "    # context = context[:16000]\n",
    "    \n",
    "    print(f\"Final context length: ~{len(context.split())} words\")\n",
    "    \n",
    "    # Step 3: Final answer\n",
    "    answer = generate_chain.invoke({\"question\": question, \"context\": context}).content\n",
    "    print(\"Answer generated!\")\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e44585",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kanth\\anaconda3\\Lib\\site-packages\\gradio\\interface.py:415: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* Running on public URL: https://38a7fbb7e4ffa1ee25.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://38a7fbb7e4ffa1ee25.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: Which magazine named the discoverer of general relativity as Person of the Century in 1999?\n",
      "Sub-questions: ['Who was named Person of the Century by a magazine in 1999?', 'What was the name of the magazine that made this designation?']\n",
      "Final context length: ~116 words\n",
      "Answer generated!\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Gradio Interface\n",
    "def gradio_fn(question):\n",
    "    return multi_hop_rag(question)\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=gradio_fn,\n",
    "    inputs=gr.Textbox(label=\"Ask a Multi-Hop Question\", lines=2, \n",
    "                      placeholder=\"e.g. Which magazine named the discoverer of general relativity Person of the Century in 1999?\"),\n",
    "    outputs=gr.Textbox(label=\"Answer\"),\n",
    "    title=\"Multi-Hop RAG with Llama3 (Ollama)\",\n",
    "    description=\"HotpotQA FullWiki • Hybrid Retrieval (FAISS + BM25) • 100% Local & Private\",\n",
    "    examples=[\n",
    "        [\"Which magazine named the discoverer of general relativity as Person of the Century in 1999?\"],\n",
    "        [\"Who directed the film that won the Academy Award for Best Picture in 1994?\"],\n",
    "        [\"Are the birthplace of Barack Obama and the capital of Hawaii the same state?\"],\n",
    "        [\"What is the name of the university whose football team is called the Crimson Tide?\"],\n",
    "    ],\n",
    "    allow_flagging=\"never\",\n",
    "    theme=gr.themes.Soft()\n",
    ")\n",
    "\n",
    "demo.launch(share=True)  # share=True → public link, share=False → localhost only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db2d89a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAST EVALUATION on 200 questions (out of 7405)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  16%|█▌        | 32/200 [01:32<08:34,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: Are the birthplace of Barack Obama and the capital of Hawaii the same state?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  20%|██        | 40/200 [04:23<1:15:04, 28.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub-questions: [\"Is Barack Obama's birthplace located in the state of Hawaii?\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  20%|██        | 41/200 [04:36<1:02:37, 23.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final context length: ~46 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  23%|██▎       | 46/200 [07:33<1:39:44, 38.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer generated!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 200/200 [17:29<00:00,  5.25s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RETRIEVAL METRICS (Sample=200 questions)\n",
      "============================================================\n",
      "Metric            K=1      K=3      K=5      K=8     K=10\n",
      "------------------------------------------------------------\n",
      "Precision@     53.50%   23.50%   15.90%   10.62%    8.80%\n",
      "Recall@        26.75%   35.25%   39.75%   42.50%   44.00%\n",
      "NDCG@          53.50%   43.14%   47.64%   50.64%   51.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MRR: 100%|██████████| 200/200 [07:03<00:00,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean Reciprocal Rank (MRR): 0.5996\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: FAST Retrieval Evaluation (sirf 200 questions pe — 10–15 min max)\n",
    "\n",
    "def evaluate_retrieval_fast(sample_size=200, top_ks=[1, 3, 5, 8, 10]):\n",
    "    print(f\"FAST EVALUATION on {sample_size} questions (out of {len(questions)})...\")\n",
    "    \n",
    "    import random\n",
    "    indices = random.sample(range(len(questions)), sample_size)\n",
    "    \n",
    "    results = defaultdict(list)\n",
    "    \n",
    "    for idx in tqdm(indices, desc=\"Evaluating\"):\n",
    "        q = questions[idx][\"question\"]\n",
    "        true_titles = get_ground_truth_titles(idx)\n",
    "        \n",
    "        retrieved_docs = retrieve(q, k=max(top_ks))\n",
    "        retrieved_titles_list = [doc.metadata[\"title\"] for doc in retrieved_docs]\n",
    "        \n",
    "        for k in top_ks:\n",
    "            retrieved_k = set(retrieved_titles_list[:k])\n",
    "            relevant_retrieved = len(retrieved_k & true_titles)\n",
    "            total_relevant = len(true_titles)\n",
    "            \n",
    "            # Precision, Recall\n",
    "            precision = relevant_retrieved / k if k > 0 else 0\n",
    "            recall = relevant_retrieved / total_relevant if total_relevant > 0 else 0\n",
    "            results[f\"Precision@{k}\"].append(precision)\n",
    "            results[f\"Recall@{k}\"].append(recall)\n",
    "            \n",
    "            # NDCG\n",
    "            dcg = sum((1 / math.log2(r+1)) for r in range(1, k+1) if retrieved_titles_list[r-1] in true_titles)\n",
    "            idcg = sum(1 / math.log2(i+1) for i in range(1, min(k, total_relevant)+1))\n",
    "            ndcg = dcg / idcg if idcg > 0 else 0\n",
    "            results[f\"NDCG@{k}\"].append(ndcg)\n",
    "    \n",
    "    # Print table\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"RETRIEVAL METRICS (Sample={sample_size} questions)\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"{'Metric':<12} {'K=1':>8} {'K=3':>8} {'K=5':>8} {'K=8':>8} {'K=10':>8}\")\n",
    "    print(\"-\"*60)\n",
    "    for metric in [\"Precision@\", \"Recall@\", \"NDCG@\"]:\n",
    "        row = f\"{metric:<12}\"\n",
    "        for k in top_ks:\n",
    "            key = f\"{metric}{k}\"\n",
    "            if key in results:\n",
    "                avg = sum(results[key]) / len(results[key])\n",
    "                row += f\"{avg*100:8.2f}%\"\n",
    "        print(row)\n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "evaluate_retrieval_fast(sample_size=200, top_ks=[1, 3, 5, 8, 10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
